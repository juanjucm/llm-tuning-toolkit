model: &model "meta-llama/Meta-Llama-3.1-8b-Instruct"
port: &port 8000
host: 0.0.0.0

# scenario config for auto-tuning
scenario:
  name: "balanced"
  prompt_options: "num_tokens=4000,min_tokens=4000,max_tokens=4000,variance=0"
  decode_options: "num_tokens=16,min_tokens=16,max_tokens=16,variance=0"
  dataset_file: null  # optional dataset file.
  throughput_duration: "60s"
  rate_duration: "60s"
  max_rate_finding_attempts: 5
  rate_decrease_factor: 0.15
  max_vus: 800
  max_model_len: &max_model_len 4096
  goodput_criteria:
    # Goodput SLOs - Performance requirements to meet for this scenario.
    # Auto-Tuning goal is to find highest throughput configuration that meet these SLOs.
    # Available values to check:
    #   max_e2e_avg_ms
    #   max_e2e_p99_ms
    #   max_ttft_avg_ms
    #   max_ttft_p99_ms
    #   max_itl_avg_ms
    #   max_itl_p99_ms
    #   min_success_rate
    #   min_throughput (req/s)
    # If not specified, no check is done for that metric.
    max_e2e_avg_ms: 10000
    min_success_rate: 0.95

# Engine to test and its parameter pool (only one engine at a time)
engine:
  name: "vllm"
  image: "vllm/vllm-openai:latest"
  base_args:
    - "--model"
    - *model
    - --port
    - *port
    - "--dtype"
    - "bfloat16"
    - "--max-model-len"
    - *max_model_len
    - "--gpu-memory-utilization"
    - "0.95"
  value_args_pool:
    # Value parameters (--param value)
    max-num-seqs: [128, 1024]
    max-num-batched-tokens: [4096]
  action_args_pool:
    # Action parameters (flags that can be included or excluded)
    enable-chunked-prefill: [true, false]