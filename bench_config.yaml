model: &model google/gemma-3-4b-it
port: &port 8000

scenarios:
  - balanced-sweep:
    description: Balanced scenario with a good compromise between concurrency and context length.
    bench_config:
      - --tokenizer-name *model
      - --benchmark-kind sweep
      - --max-vus 128
      - --prompt-options "num_tokens=1500,min_tokens=100,max_tokens=8000,variance=500"
      - --decode-options "num_tokens=192,min_tokens=192,max_tokens=192,variance=0"
      - --duration "120s"
      - --warmup "30s"
    engines:
      - vLLM:
        image: vllm/vllm-openai:latest
        args:
          - --model *model
          - --dtype bfloat16  
          - --max_model_len 8192 
          - --gpu_memory_utilization 0.9
      - TGI:
        image: ghcr.io/huggingface/text-generation-inference:3.2.3
        envs: 
          - PREFIX_CACHING=0
        args:
          - --model-id *model 
          - --dtype bfloat16 
          - --num-shard 1 
          - --max-total-tokens 8192 
      - SGLang:
        image: lmsysorg/sglang:dev
        args:
          - python3 -m sglang.launch_server 
          - --host 0.0.0.0 
          - --port *port 
          - --model-path *model 
          - --dtype bfloat16 
          - --context-length 8192 
          - --disable-cuda-graph 
          - --mem-fraction-static 0.7 
          - --disable-overlap-schedule 
          - --enable-p2p-check  
  - high_context:
    description: High context scenario focused on testing long context handling capabilities.
    bench_config:
      - --tokenizer-name *model
      - --benchmark-kind sweep
      - --max-vus 32
      - --prompt-options "num_tokens=6000,min_tokens=4000,max_tokens=7500,variance=1000"
      - --decode-options "num_tokens=512,min_tokens=256,max_tokens=1024,variance=128"
      - --duration "180s"
      - --warmup "45s"
    engines:
      - vLLM:
        image: vllm/vllm-openai:latest
        args:
          - --model google/gemma-3-4b-it 
          - --dtype bfloat16  
          - --max_model_len 8192 
          - --gpu_memory_utilization 0.85
          - --enable-chunked-prefill
      - TGI:
        image: ghcr.io/huggingface/text-generation-inference:latest
        envs: 
          - PREFIX_CACHING=1
        args:
          - --model-id *model 
          - --dtype bfloat16 
          - --num-shard 1 
          - --max-total-tokens 8192 
          - --max-input-length 7500
      - SGLang:
        image: lmsysorg/sglang:dev
        args:
          - python3 -m sglang.launch_server 
          - --host 0.0.0.0 
          - --port *port 
          - --model-path *model 
          - --dtype bfloat16 
          - --context-length 8192 
          - --disable-cuda-graph 
          - --mem-fraction-static 0.8 
          - --enable-overlap-schedule 
          - --enable-p2p-check
  - high_concurrency:
    description: High concurrency scenario focused on testing maximum throughput with many concurrent users.
    bench_config:
      - --tokenizer-name *model
      - --benchmark-kind sweep
      - --max-vus 512
      - --prompt-options "num_tokens=800,min_tokens=200,max_tokens=1500,variance=300"
      - --decode-options "num_tokens=128,min_tokens=64,max_tokens=256,variance=32"
      - --duration "240s"
      - --warmup "60s"
    engines:
      - vLLM:
        image: vllm/vllm-openai:latest
        args:
          - --model google/gemma-3-4b-it 
          - --dtype bfloat16  
          - --max_model_len 4096 
          - --gpu_memory_utilization 0.95
          - --max-num-seqs 256
          - --enable-chunked-prefill
      - TGI:
        image: ghcr.io/huggingface/text-generation-inference:latest
        envs: 
          - PREFIX_CACHING=1
        args:
          - --model-id *model 
          - --dtype bfloat16 
          - --num-shard 1 
          - --max-total-tokens 4096 
          - --max-concurrent-requests 128
          - --max-batch-prefill-tokens 2048
      - SGLang:
        image: lmsysorg/sglang:dev
        args:
          - python3 -m sglang.launch_server 
          - --host 0.0.0.0 
          - --port *port 
          - --model-path *model 
          - --dtype bfloat16 
          - --context-length 4096 
          - --enable-cuda-graph 
          - --mem-fraction-static 0.6 
          - --enable-overlap-schedule 
          - --enable-p2p-check 
          - --schedule-policy fcfs

